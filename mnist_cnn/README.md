这个文件夹主要是关于CNN中的LeNet-5模型的一个**类似**实现，例子仍然是深度为1的mnist数据集的数字识别。

首先看下神经网络构成：

| 层数             | 层数规格                                   | 输入矩阵规格   | 输出矩阵规格   |
| ---------------- | ------------------------------------------ | -------------- | -------------- |
| 第一层：卷积层   | （过滤器尺寸， 深度）= （5, 5, 32）步长：1 | （28，28，1）  | （28，28，32） |
| 第二层：池化层   | 过滤器尺寸 (2, 2)： 步长：2                | （28，28, 32） | (14, 14, 32)   |
| 第三层：卷积层   | （过滤器尺寸， 深度）= （5, 5, 64）步长：1 | （14，14，32） | （14，14，64） |
| 第四层：池化层   | 过滤器尺寸 (2, 2)： 步长：2                | （14，14，64） | （7，7，64）   |
| 第五层：全连接层 | 权重向量维度：（3136，512）                | 3136           | 512            |
| 第六层：全连接层 | 权重向量维度：（512，10）                  | 512            | 10             |

**注意** ：这里的神经网络结构并不是与LeNet-5完全相同，这里的结构少了最后的径向基函数那一层，而且过滤器的深度也不一样。

**关于tensorboard**：经过试验发现，在每一轮训练中，使用tf.summary来记录损失函数和学习率，会大大减慢运行速度，所以如果想要运行时间短一点，可以每隔1000步来打印一次损失函数和学习率，而不是用tensorboard可视化。